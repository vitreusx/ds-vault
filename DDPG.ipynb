{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXR2qlEU-e_R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda_if_available = True\n",
        "use_cuda = use_cuda_if_available and torch.cuda.is_available()\n",
        "DEVICE = torch.device('cuda' if use_cuda else 'cpu')\n",
        "print(f\"Using CUDA: {use_cuda}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20kznQYx_SxF",
        "outputId": "a6de31fe-6819-4945-c1f4-75df05a2ff49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import contextmanager\n",
        "\n",
        "def as_tensor(data: np.ndarray, dtype=torch.float32, batch: bool = False) -> torch.Tensor:\n",
        "  tensor = torch.as_tensor(data, dtype=dtype, device=DEVICE)\n",
        "  if batch and len(tensor.shape) == 1:\n",
        "    tensor = tensor.unsqueeze(dim=0)\n",
        "  return tensor\n",
        "\n",
        "@contextmanager\n",
        "def eval_mode(net: nn.Module):\n",
        "  training_before = net.training\n",
        "  try:\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "      yield net\n",
        "  finally:\n",
        "    if training_before:\n",
        "      net.train()"
      ],
      "metadata": {
        "id": "aYOTJlcK__jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "def set_seeds(seed=42, env: gym.Env = None):\n",
        "  torch.use_deterministic_algorithms(mode=True)\n",
        "  torch.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if env:\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)"
      ],
      "metadata": {
        "id": "jXxV4spyzX18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "from typing import Tuple\n",
        "\n",
        "Transition = namedtuple(\"Transition\", (\n",
        "    \"state\", \"action\", \"reward\", \"next_state\", \"done\"))\n",
        "\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, env: gym.Env, capacity: int):\n",
        "    self.size = 0\n",
        "    self.capacity = capacity\n",
        "    self.cur_slot = 0\n",
        "\n",
        "    num_states = np.prod(env.observation_space.shape)\n",
        "    num_actions = np.prod(env.action_space.shape)\n",
        "\n",
        "    def make_buf(dim):\n",
        "      return as_tensor(torch.empty(self.capacity, dim))\n",
        "\n",
        "    self.state_buf = make_buf(num_states)\n",
        "    self.action_buf = make_buf(num_actions)\n",
        "    self.reward_buf = make_buf(1)\n",
        "    self.next_state_buf = make_buf(num_states)\n",
        "    self.done_buf = make_buf(1)\n",
        "  \n",
        "  def record(self, tr: Transition):\n",
        "    self.state_buf[self.cur_slot] = as_tensor(tr.state).view(-1)\n",
        "    self.action_buf[self.cur_slot] = as_tensor(tr.action).view(-1)\n",
        "    self.reward_buf[self.cur_slot] = tr.reward\n",
        "    self.next_state_buf[self.cur_slot] = as_tensor(tr.next_state).view(-1)\n",
        "    self.done_buf[self.cur_slot] = tr.done\n",
        "\n",
        "    self.size = max(self.size, self.cur_slot+1)\n",
        "    self.cur_slot = (self.cur_slot + 1) % self.capacity\n",
        "\n",
        "  def sample(self, batch_size: int) -> Transition:\n",
        "    indices = torch.randint(self.size, (batch_size,))\n",
        "    state_batch = self.state_buf[indices]\n",
        "    action_batch = self.action_buf[indices]\n",
        "    reward_batch = self.reward_buf[indices]\n",
        "    next_state_batch = self.next_state_buf[indices]\n",
        "    done_batch = self.done_buf[indices]\n",
        "    \n",
        "    return Transition(state_batch, action_batch, reward_batch,\n",
        "                      next_state_batch, done_batch)"
      ],
      "metadata": {
        "id": "yfHbn_utAUR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OUNoise:\n",
        "  def __init__(self, mean=0, std=0.2, theta=0.15, dt=1e-2):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "    self.theta = theta\n",
        "    self.dt = dt\n",
        "    self.x = mean\n",
        "  \n",
        "  def step(self, n=1):\n",
        "    w = np.random.normal(size=n)\n",
        "    res = np.empty(n)\n",
        "    sqrt_dt = np.sqrt(self.dt)\n",
        "    for idx in range(n):\n",
        "      res[idx] = self.x\n",
        "      dx = self.theta * (self.mean - self.x) * self.dt + \\\n",
        "        self.std * sqrt_dt * w[idx]\n",
        "      self.x += dx\n",
        "\n",
        "    return res if n > 1 else res[0]"
      ],
      "metadata": {
        "id": "WDx-6TVVSf01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "\n",
        "def unif_init(fc: nn.Linear, a: float):\n",
        "  nn.init.uniform_(fc.weight.data, -a, a)\n",
        "  nn.init.uniform_(fc.bias.data, -a, a)\n",
        "\n",
        "def fanin_init(fc: nn.Linear):\n",
        "  a = 1/np.sqrt(fc.weight.data.size()[0])\n",
        "  unif_init(fc, a)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self, env: gym.Env, hidden1=400, hidden2=300,\n",
        "               final_layer_a=3e-3):\n",
        "    super(Actor, self).__init__()\n",
        "\n",
        "    self.env = env\n",
        "    self.num_states = np.prod(env.observation_space.shape)\n",
        "    self.num_actions = np.prod(env.action_space.shape)\n",
        "\n",
        "    self.action_loc = (env.action_space.high + env.action_space.low) / 2.0\n",
        "    self.action_loc = as_tensor(self.action_loc)\n",
        "\n",
        "    self.action_scale = (env.action_space.high - env.action_space.low) / 2.0\n",
        "    self.action_scale = as_tensor(self.action_scale)\n",
        "\n",
        "    self.bn0 = nn.BatchNorm1d(self.num_states)\n",
        "    self.fc1 = nn.Linear(self.num_states, hidden1)\n",
        "    self.bn1 = nn.BatchNorm1d(hidden1)\n",
        "    self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "    self.bn2 = nn.BatchNorm1d(hidden2)\n",
        "    self.final = nn.Linear(hidden2, self.num_actions)\n",
        "\n",
        "    fanin_init(self.fc1)\n",
        "    fanin_init(self.fc2)\n",
        "    unif_init(self.final, final_layer_a)\n",
        "  \n",
        "  def forward(self, s):\n",
        "    out = self.bn0(s)\n",
        "    out = self.bn1(func.relu(self.fc1(out)))\n",
        "    out = self.bn2(func.relu(self.fc2(out)))\n",
        "    out = torch.tanh(self.final(out))\n",
        "    out = self.action_scale * out + self.action_loc\n",
        "    return out\n",
        "  \n",
        "  def policy(self, s, noise=None):\n",
        "    with eval_mode(self):\n",
        "      s = as_tensor(np.ravel(s), batch=True)\n",
        "      action = self.forward(s)[0]\n",
        "      action = action.cpu().numpy()\n",
        "      if noise:\n",
        "        action += noise\n",
        "        action = np.clip(action, self.env.action_space.low, self.env.action_space.high)\n",
        "      return np.reshape(action, self.env.action_space.shape)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, env: gym.Env, hidden1=400, hidden2=300,\n",
        "               final_layer_a=3e-3):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    self.env = env\n",
        "    num_states = np.prod(env.observation_space.shape)\n",
        "    num_actions = np.prod(env.action_space.shape)\n",
        "\n",
        "    self.bn0 = nn.BatchNorm1d(num_states)\n",
        "    self.fc1 = nn.Linear(num_states, hidden1)\n",
        "    self.bn1 = nn.BatchNorm1d(hidden1)\n",
        "    self.fc2 = nn.Linear(hidden1 + num_actions, hidden2)\n",
        "    self.final = nn.Linear(hidden2, 1)\n",
        "\n",
        "    fanin_init(self.fc1)\n",
        "    fanin_init(self.fc2)\n",
        "    unif_init(self.final, final_layer_a)\n",
        "\n",
        "  def forward(self, s, a):\n",
        "    out = self.bn0(s)\n",
        "    out = self.bn1(func.relu(self.fc1(out)))\n",
        "    out = torch.cat((out, a), dim=1)\n",
        "    out = func.relu(self.fc2(out))\n",
        "    out = self.final(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "gl_iXNTCHqT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from typing import Iterable\n",
        "\n",
        "def make_episode(env: gym.Env, action_source) -> Iterable[Transition]:\n",
        "  state = env.reset()\n",
        "  for step in itertools.count():\n",
        "    action = action_source(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    yield Transition(state, action, reward, next_state, done)\n",
        "    if done: break\n",
        "    state = next_state\n",
        "\n",
        "def create_replay_buffer(env: gym.Env, capacity: int, prefill: int):\n",
        "  replay_buffer = ReplayBuffer(env, capacity)\n",
        "  episode = None\n",
        "\n",
        "  def random_action_source(state):\n",
        "    return env.action_space.sample()\n",
        "  \n",
        "  while replay_buffer.size < prefill:\n",
        "    if episode is None:\n",
        "      episode = make_episode(env, random_action_source)\n",
        "    \n",
        "    try:\n",
        "      transition = next(episode)\n",
        "      replay_buffer.record(transition)\n",
        "    except StopIteration:\n",
        "      episode = None\n",
        "  \n",
        "  return replay_buffer\n",
        "\n",
        "def update_target(target_net: nn.Module, source_net: nn.Module, tau: float):\n",
        "  target_params = target_net.state_dict()\n",
        "  source_params = source_net.state_dict()\n",
        "\n",
        "  for name, source_param in source_params.items():\n",
        "    if name in target_params:\n",
        "      target_param = target_params[name]\n",
        "      polyak_avg = tau*source_param.data+(1-tau)*target_param.data\n",
        "      target_params[name].data.copy_(polyak_avg)\n",
        "    \n",
        "  target_net.load_state_dict(target_params)"
      ],
      "metadata": {
        "id": "-IhhBw82agLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "StepStats = namedtuple(\"StepStats\", (\n",
        "    \"overall_step\", \"episode_num\", \"step\", \"transition\",\n",
        "    \"step_actor_loss\", \"step_critic_loss\"))\n",
        "\n",
        "EpisodeStats = namedtuple(\"EpisodeStats\", (\n",
        "    \"episode_num\", \"reward\"\n",
        "))\n",
        "\n",
        "def train(env: gym.Env, actor: Actor, critic: Critic, actor_optim, critic_optim,\n",
        "          noise: OUNoise, replay_buffer: ReplayBuffer, batch_size, gamma, tau,\n",
        "          on_step=None, on_episode_end=None):\n",
        "  target_actor = copy.deepcopy(actor)\n",
        "  target_actor.eval()\n",
        "\n",
        "  target_critic = copy.deepcopy(critic)\n",
        "  target_critic.eval()\n",
        "\n",
        "  def critic_loss(batch: Transition):\n",
        "    with torch.no_grad():\n",
        "      next_action = target_actor(batch.next_state)\n",
        "      next_q_value = target_critic(batch.next_state, next_action)\n",
        "      target = batch.reward + gamma * (1 - batch.done) * next_q_value\n",
        "    value_estimate = critic(batch.state, batch.action)\n",
        "    return func.mse_loss(value_estimate, target)\n",
        "  \n",
        "  def actor_loss(batch: Transition):\n",
        "    q_value = critic(batch.state, actor(batch.state))\n",
        "    return -torch.mean(q_value)\n",
        "  \n",
        "  def update_step():\n",
        "    batch = replay_buffer.sample(batch_size)\n",
        "\n",
        "    critic.zero_grad()\n",
        "    step_critic_loss = critic_loss(batch)\n",
        "    step_critic_loss.backward()\n",
        "    critic_optim.step()\n",
        "\n",
        "    actor.zero_grad()\n",
        "    step_actor_loss = actor_loss(batch)\n",
        "    step_actor_loss.backward()\n",
        "    actor_optim.step()\n",
        "\n",
        "    update_target(target_actor, actor, tau)\n",
        "    update_target(target_critic, critic, tau)\n",
        "    return step_actor_loss.item(), step_critic_loss.item()\n",
        "\n",
        "  def action_source(state):\n",
        "    return actor.policy(state, noise.step())\n",
        "\n",
        "  overall_step = 0\n",
        "  for episode_num in itertools.count():\n",
        "    episode = None\n",
        "    episode_reward = 0\n",
        "\n",
        "    for step in itertools.count():\n",
        "      if episode is None:\n",
        "        episode = make_episode(env, action_source)\n",
        "\n",
        "      try:\n",
        "        transition: Transition = next(episode)\n",
        "        episode_reward += transition.reward\n",
        "        replay_buffer.record(transition)\n",
        "        step_actor_loss, step_critic_loss = update_step()\n",
        "\n",
        "        if on_step:\n",
        "          stats = StepStats(overall_step, episode_num, step, transition,\n",
        "                            step_actor_loss, step_critic_loss)\n",
        "          if on_step(stats):\n",
        "            return\n",
        "\n",
        "      except StopIteration:\n",
        "        episode = None\n",
        "        if on_episode_end:\n",
        "          stats = EpisodeStats(episode_num, episode_reward)\n",
        "          if on_episode_end(stats):\n",
        "            return\n",
        "        break\n",
        "\n",
        "      finally:\n",
        "        overall_step += 1"
      ],
      "metadata": {
        "id": "y3jKzfZXIHNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark(env: gym.Env, actor: Actor, total_episodes=64):\n",
        "  episode_rewards = np.empty(total_episodes)\n",
        "\n",
        "  def action_source(state):\n",
        "    return actor.policy(state)\n",
        "\n",
        "  set_seeds(env=env)\n",
        "  for episode_num in range(total_episodes):\n",
        "    episode_reward = 0\n",
        "    for transition in make_episode(env, action_source):\n",
        "      episode_reward += transition.reward\n",
        "    episode_rewards[episode_num] = episode_reward\n",
        "  \n",
        "  return np.mean(episode_rewards)"
      ],
      "metadata": {
        "id": "o4wmb7lPy8Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_until_sufficient(env_id: str, req_score: float):\n",
        "  env = gym.make(env_id)\n",
        "  actor = Actor(env).to(DEVICE)\n",
        "  critic = Critic(env).to(DEVICE)\n",
        "  actor_optim = torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
        "  critic_optim = torch.optim.Adam(critic.parameters(), \n",
        "                                  lr=1e-4, weight_decay=1e-2)\n",
        "  noise = OUNoise()\n",
        "  replay_buffer = create_replay_buffer(env, int(1e6), int(1e3))\n",
        "  batch_size = 64\n",
        "  gamma = 1-1e-2\n",
        "  tau = 1e-3\n",
        "\n",
        "  benchmark_period = 16\n",
        "  benchmark_size = 128\n",
        "\n",
        "  episode_rewards = []\n",
        "  def on_episode_end(stats: EpisodeStats):\n",
        "    episode_rewards.append(stats.reward)\n",
        "    mean_reward = np.mean(episode_rewards[-32:])\n",
        "    print(f\"Episode #{stats.episode_num}. Mean reward = {mean_reward}\")\n",
        "    if stats.episode_num > 0 and stats.episode_num % benchmark_period == 0:\n",
        "      score = benchmark(env, actor, total_episodes=benchmark_size)\n",
        "      print(f\"[Benchmark] Score: {score}\")\n",
        "      if score > req_score:\n",
        "        return True\n",
        "\n",
        "  train(env, actor, critic, actor_optim, critic_optim, noise, replay_buffer,\n",
        "        batch_size, gamma, tau, on_episode_end=on_episode_end)\n",
        "  \n",
        "  return actor"
      ],
      "metadata": {
        "id": "pgCSHMItaItC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_until_sufficient(\"Pendulum-v1\", -160)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "88dusU4_1MZF",
        "outputId": "8363933f-b0c2-44d1-cbb0-933a683661b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode #0. Mean reward = -1203.1900336244596\n",
            "Episode #1. Mean reward = -1249.3253867591625\n",
            "Episode #2. Mean reward = -1357.321775632476\n",
            "Episode #3. Mean reward = -1347.8823730187823\n",
            "Episode #4. Mean reward = -1402.2891948453982\n",
            "Episode #5. Mean reward = -1439.6136207705115\n",
            "Episode #6. Mean reward = -1452.9114968540562\n",
            "Episode #7. Mean reward = -1490.2073530738626\n",
            "Episode #8. Mean reward = -1514.6910180677291\n",
            "Episode #9. Mean reward = -1522.4171326960243\n",
            "Episode #10. Mean reward = -1542.2024431460952\n",
            "Episode #11. Mean reward = -1565.94386172617\n",
            "Episode #12. Mean reward = -1571.2318761419067\n",
            "Episode #13. Mean reward = -1587.86450397867\n",
            "Episode #14. Mean reward = -1593.874195859919\n",
            "Episode #15. Mean reward = -1606.3637045254845\n",
            "Episode #16. Mean reward = -1619.1206480854248\n",
            "[Benchmark] Score: -1770.766601195583\n",
            "Episode #17. Mean reward = -1628.4890640863985\n",
            "Episode #18. Mean reward = -1638.416776483169\n",
            "Episode #19. Mean reward = -1648.4736423368347\n",
            "Episode #20. Mean reward = -1656.1427970687841\n",
            "Episode #21. Mean reward = -1655.2215411478683\n",
            "Episode #22. Mean reward = -1664.2225270292015\n",
            "Episode #23. Mean reward = -1671.6430615988381\n",
            "Episode #24. Mean reward = -1678.0670015057526\n",
            "Episode #25. Mean reward = -1678.1871061097288\n",
            "Episode #26. Mean reward = -1683.2826540815836\n",
            "Episode #27. Mean reward = -1687.4034120994904\n",
            "Episode #28. Mean reward = -1691.0891460935868\n",
            "Episode #29. Mean reward = -1691.0735850371775\n",
            "Episode #30. Mean reward = -1693.5230748776185\n",
            "Episode #31. Mean reward = -1686.1098061754508\n",
            "Episode #32. Mean reward = -1698.7692019623184\n",
            "[Benchmark] Score: -1595.96933637261\n",
            "Episode #33. Mean reward = -1709.170771532542\n",
            "Episode #34. Mean reward = -1715.1423104038934\n",
            "Episode #35. Mean reward = -1724.769677121157\n",
            "Episode #36. Mean reward = -1725.8695664855045\n",
            "Episode #37. Mean reward = -1723.5482759926253\n",
            "Episode #38. Mean reward = -1730.440660132839\n",
            "Episode #39. Mean reward = -1726.72810790661\n",
            "Episode #40. Mean reward = -1723.4450930288024\n",
            "Episode #41. Mean reward = -1723.5992840615227\n",
            "Episode #42. Mean reward = -1718.794528017263\n",
            "Episode #43. Mean reward = -1713.0305148642465\n",
            "Episode #44. Mean reward = -1711.308299150413\n",
            "Episode #45. Mean reward = -1700.3657283169362\n",
            "Episode #46. Mean reward = -1698.3445596596266\n",
            "Episode #47. Mean reward = -1678.4841967031625\n",
            "Episode #48. Mean reward = -1667.3175184599054\n",
            "[Benchmark] Score: -1438.7926557803698\n",
            "Episode #49. Mean reward = -1657.5085689667192\n",
            "Episode #50. Mean reward = -1649.0038687757146\n",
            "Episode #51. Mean reward = -1639.573932214515\n",
            "Episode #52. Mean reward = -1630.8665033742539\n",
            "Episode #53. Mean reward = -1625.5402661667927\n",
            "Episode #54. Mean reward = -1614.3329130194095\n",
            "Episode #55. Mean reward = -1603.7021004567032\n",
            "Episode #56. Mean reward = -1593.8076433367114\n",
            "Episode #57. Mean reward = -1585.1805991731114\n",
            "Episode #58. Mean reward = -1575.4822009237405\n",
            "Episode #59. Mean reward = -1566.1416154726253\n",
            "Episode #60. Mean reward = -1558.0892468409143\n",
            "Episode #61. Mean reward = -1548.1446311239315\n",
            "Episode #62. Mean reward = -1540.953020874243\n",
            "Episode #63. Mean reward = -1523.2656244523607\n",
            "Episode #64. Mean reward = -1517.6700110536385\n",
            "[Benchmark] Score: -1355.1322023216426\n",
            "Episode #65. Mean reward = -1514.413738447379\n",
            "Episode #66. Mean reward = -1507.0747812762338\n",
            "Episode #67. Mean reward = -1504.5910860002352\n",
            "Episode #68. Mean reward = -1501.2158466342714\n",
            "Episode #69. Mean reward = -1496.8977837957304\n",
            "Episode #70. Mean reward = -1489.1163451935108\n",
            "Episode #71. Mean reward = -1489.4046524975124\n",
            "Episode #72. Mean reward = -1486.6366912407643\n",
            "Episode #73. Mean reward = -1479.7142467693368\n",
            "Episode #74. Mean reward = -1477.1010419940192\n",
            "Episode #75. Mean reward = -1472.635857585795\n",
            "Episode #76. Mean reward = -1471.0272291763595\n",
            "Episode #77. Mean reward = -1468.7145329306175\n",
            "Episode #78. Mean reward = -1465.7508626857666\n",
            "Episode #79. Mean reward = -1435.0491644089916\n",
            "Episode #80. Mean reward = -1433.450997101032\n",
            "[Benchmark] Score: -1344.7727498855847\n",
            "Episode #81. Mean reward = -1435.0266353084178\n",
            "Episode #82. Mean reward = -1434.5451353220974\n",
            "Episode #83. Mean reward = -1434.8172685589957\n",
            "Episode #84. Mean reward = -1435.1065993616326\n",
            "Episode #85. Mean reward = -1432.1008034896747\n",
            "Episode #86. Mean reward = -1432.1184427393591\n",
            "Episode #87. Mean reward = -1432.1499161250372\n",
            "Episode #88. Mean reward = -1432.161381191253\n",
            "Episode #89. Mean reward = -1431.015960035866\n",
            "Episode #90. Mean reward = -1430.9262634062902\n",
            "Episode #91. Mean reward = -1430.9262634062902\n",
            "Episode #92. Mean reward = -1431.3592480833163\n",
            "Episode #93. Mean reward = -1430.8662000078243\n",
            "Episode #94. Mean reward = -1430.291787358682\n",
            "Episode #95. Mean reward = -1441.8536539898346\n",
            "Episode #96. Mean reward = -1441.1776466476608\n",
            "[Benchmark] Score: -1355.9130145606846\n",
            "Episode #97. Mean reward = -1441.1832046316863\n",
            "Episode #98. Mean reward = -1441.183205341465\n",
            "Episode #99. Mean reward = -1441.0500253537316\n",
            "Episode #100. Mean reward = -1440.8729669880368\n",
            "Episode #101. Mean reward = -1441.4455372056937\n",
            "Episode #102. Mean reward = -1441.4431734907262\n",
            "Episode #103. Mean reward = -1437.077795743441\n",
            "Episode #104. Mean reward = -1437.0702162525922\n",
            "Episode #105. Mean reward = -1437.0123504657254\n",
            "Episode #106. Mean reward = -1437.008380377149\n",
            "Episode #107. Mean reward = -1437.008380377149\n",
            "Episode #108. Mean reward = -1436.3822975563533\n",
            "Episode #109. Mean reward = -1435.7689954938917\n",
            "Episode #110. Mean reward = -1435.7689954938915\n",
            "Episode #111. Mean reward = -1441.0471115290525\n",
            "Episode #112. Mean reward = -1441.3725544503814\n",
            "[Benchmark] Score: -1420.338847478672\n",
            "Episode #113. Mean reward = -1441.6616023308807\n",
            "Episode #114. Mean reward = -1441.661602402768\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_until_sufficient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPendulum-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mtrain_until_sufficient\u001b[0;34m(env_id, req_score)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m req_score:\n\u001b[1;32m     26\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_episode_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_episode_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actor\n",
            "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, actor, critic, actor_optim, critic_optim, noise, replay_buffer, batch_size, gamma, tau, on_step, on_episode_end)\u001b[0m\n\u001b[1;32m     63\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transition\u001b[38;5;241m.\u001b[39mreward\n\u001b[1;32m     64\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mrecord(transition)\n\u001b[0;32m---> 65\u001b[0m step_actor_loss, step_critic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_step:\n\u001b[1;32m     68\u001b[0m   stats \u001b[38;5;241m=\u001b[39m StepStats(overall_step, episode_num, step, transition,\n\u001b[1;32m     69\u001b[0m                     step_actor_loss, step_critic_loss)\n",
            "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain.<locals>.update_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m step_actor_loss \u001b[38;5;241m=\u001b[39m actor_loss(batch)\n\u001b[1;32m     42\u001b[0m step_actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 43\u001b[0m \u001b[43mactor_optim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m update_target(target_actor, actor, tau)\n\u001b[1;32m     46\u001b[0m update_target(target_critic, critic, tau)\n",
            "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/optim/adam.py:133\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/venv/lib/python3.8/site-packages/torch/optim/_functional.py:86\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     83\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}